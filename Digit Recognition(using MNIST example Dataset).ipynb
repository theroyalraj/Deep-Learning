{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/theroyalraj/Deep-Learning/blob/master/Digit%20Recognition(using%20MNIST%20example%20Dataset).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit rcognition (MNIST Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "colab_type": "code",
    "id": "s2FYndp3Fepq",
    "outputId": "0019b490-6056-420c-d371-e514c040f6c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-03e2ed9757cd>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\Raj\\Anaconda3_1\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\Raj\\Anaconda3_1\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Raj\\Anaconda3_1\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Raj\\Anaconda3_1\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Raj\\Anaconda3_1\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Raj\\Anaconda3_1\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "# Use same hidden layer size for both hidden layers. Not a necessity.\n",
    "hidden_layer_size = 50\n",
    "\n",
    "# Reset any variables left in memory from previous runs.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# As in the previous example - declare placeholders where the data will be fed into.\n",
    "inputs = tf.placeholder(tf.float32, [None, input_size])\n",
    "targets = tf.placeholder(tf.float32, [None, output_size])\n",
    "\n",
    "# Weights and biases for the first linear combination between the inputs and the first hidden layer.\n",
    "# Use get_variable in order to make use of the default TensorFlow initializer which is Xavier.\n",
    "weights_1 = tf.get_variable(\"weights_1\", [input_size, hidden_layer_size])\n",
    "biases_1 = tf.get_variable(\"biases_1\", [hidden_layer_size])\n",
    "\n",
    "# Operation between the inputs and the first hidden layer.\n",
    "# We've chosen ReLu as our activation function. You can try playing with different non-linearities.\n",
    "outputs_1 = tf.nn.relu(tf.matmul(inputs, weights_1) + biases_1)\n",
    "\n",
    "# Weights and biases for the second linear combination.\n",
    "# This is between the first and second hidden layers.\n",
    "weights_2 = tf.get_variable(\"weights_2\", [hidden_layer_size, hidden_layer_size])\n",
    "biases_2 = tf.get_variable(\"biases_2\", [hidden_layer_size])\n",
    "\n",
    "# Operation between the first and the second hidden layers. Again, we use ReLu.\n",
    "outputs_2 = tf.nn.relu(tf.matmul(outputs_1, weights_2) + biases_2)\n",
    "\n",
    "weights_3 = tf.get_variable(\"weights_3\", [hidden_layer_size, hidden_layer_size])\n",
    "biases_3 = tf.get_variable(\"biases_3\", [hidden_layer_size])\n",
    "\n",
    "# Operation between the second and the third hidden layers. Again, we use ReLu.\n",
    "outputs_3 = tf.nn.relu(tf.matmul(outputs_2, weights_3) + biases_3)\n",
    "\n",
    "# Weights and biases for the final linear combination.\n",
    "# That's between the second hidden layer and the output layer.\n",
    "weights_4 = tf.get_variable(\"weights_4\", [hidden_layer_size, output_size])\n",
    "biases_4 = tf.get_variable(\"biases_4\", [output_size])\n",
    "\n",
    "outputs=tf.matmul(outputs_3,weights_4)+biases_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating Loss Function\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=outputs, labels=targets)\n",
    "# Get the Mean loss\n",
    "mean_loss = tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimization step. Using adaptive optimizers such as Adam in TensorFlow\n",
    "# Tensorflow has in-built function for otimizer we will use AdamOptimzer\n",
    "optimize = tf.train.AdamOptimizer(learning_rate=0.001).minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Accuracy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a 0 or 1 for every input in the batch indicating whether it output the correct answer out of the 10.\n",
    "out_equals_target = tf.equal(tf.argmax(outputs, axis=1), tf.argmax(targets, axis=1))\n",
    "\n",
    "# Get the average accuracy of the outputs.\n",
    "accuracy = tf.reduce_mean(tf.cast(out_equals_target, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the session variable.\n",
    "sess = tf.InteractiveSession()\n",
    "initializer = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now running the initializer sesssion\n",
    "# It will initialize all the variables using Xavier Initiolize method. \n",
    "sess.run(initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Batch Size \n",
    "batch_size = 50\n",
    "\n",
    "# Number of batches per epoch.\n",
    "batches_number = mnist.train._num_examples // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "4c4iTtWAFgmr",
    "outputId": "80d1226b-3386-481b-d913-3f1710cc5801"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Mean loss: 0.3419. Validation loss: 0.1593. Validation accuracy: 95.500%\n",
      "Epoch 2. Mean loss: 0.1514. Validation loss: 0.1247. Validation accuracy: 96.160%\n",
      "Epoch 3. Mean loss: 0.1122. Validation loss: 0.1146. Validation accuracy: 96.440%\n",
      "Epoch 4. Mean loss: 0.0908. Validation loss: 0.1119. Validation accuracy: 96.680%\n",
      "Epoch 5. Mean loss: 0.0768. Validation loss: 0.1116. Validation accuracy: 96.980%\n",
      "Epoch 6. Mean loss: 0.0655. Validation loss: 0.0976. Validation accuracy: 97.180%\n",
      "Epoch 7. Mean loss: 0.0563. Validation loss: 0.0915. Validation accuracy: 97.520%\n",
      "Epoch 8. Mean loss: 0.0506. Validation loss: 0.0859. Validation accuracy: 97.900%\n",
      "Epoch 9. Mean loss: 0.0438. Validation loss: 0.0883. Validation accuracy: 97.640%\n",
      "End of training.\n"
     ]
    }
   ],
   "source": [
    "# Setting A Max limit.\n",
    "max_epochs = 25\n",
    "\n",
    "#for early stopping we need to check weather the validation loss start increasing.\n",
    "prev_validation_loss = float('inf')\n",
    "\n",
    "\n",
    "#loop for epoch\n",
    "for e in range(max_epochs):\n",
    "    \n",
    "    # Keep track of the sum of batch losses in the epoch.\n",
    "    curr_epoch_loss = 0.\n",
    "    \n",
    "    # Iterate over the batches in this epoch.\n",
    "    for batch_counter in range(batches_number):\n",
    "        \n",
    "        # Input batch and target batch are assigned values from the train dataset, given a batch size\n",
    "        input_batch, target_batch = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        # Run the optimization step and get the mean loss for this batch.\n",
    "        _, batch_loss = sess.run([optimize, mean_loss], \n",
    "            feed_dict={inputs: input_batch, targets: target_batch})\n",
    "        \n",
    "        curr_epoch_loss += batch_loss\n",
    "    \n",
    "    #average batch loss\n",
    "    curr_epoch_loss /= batches_number\n",
    "    \n",
    "    # At the end of each epoch, get the validation loss and accuracy\n",
    "    # Get the input batch and the target batch from the validation dataset\n",
    "    input_batch, target_batch = mnist.validation.next_batch(mnist.validation._num_examples)\n",
    "    \n",
    "    # Run without the optimization step (simply forward propagate)\n",
    "    validation_loss, validation_accuracy = sess.run([mean_loss, accuracy], \n",
    "        feed_dict={inputs: input_batch, targets: target_batch})\n",
    "    \n",
    "    #Print the current Epoch situation\n",
    "    print('Epoch '+str(e+1)+\n",
    "          '. Mean loss: '+'{0:.4f}'.format(curr_epoch_loss)+\n",
    "          '. Validation loss: '+'{0:.4f}'.format(validation_loss)+\n",
    "          '. Validation accuracy: '+'{0:.3f}'.format(validation_accuracy * 100.)+'%')\n",
    "    \n",
    "    # For stopping if validation loss starts increasing to avoid the overfitting\n",
    "    if validation_loss > prev_validation_loss:\n",
    "        break\n",
    "        \n",
    "    # updateing prev validation loss.\n",
    "    prev_validation_loss = validation_loss\n",
    "print('End of training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ki7Emhx1FlAe",
    "outputId": "9dfa3feb-e74d-4e6c-9520-6d1f23056211"
   },
   "outputs": [],
   "source": [
    "input_batch, target_batch = mnist.test.next_batch(mnist.test._num_examples) \n",
    "test_accuracy = sess.run([accuracy], \n",
    "                          feed_dict={inputs: input_batch, targets: target_batch})   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 97.21999764442444%\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy = \"+str(test_accuracy[0]*100)+\" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizating the Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.68235296 0.40000004 0.40000004 0.09803922 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.09803922 0.38431376 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.30980393 0.9960785\n",
      " 0.9960785  0.83921576 0.79215693 0.79215693 0.34901962 0.18431373\n",
      " 0.18431373 0.18431373 0.18431373 0.3529412  0.79215693 0.79215693\n",
      " 0.8431373  0.9921569  0.79215693 0.14117648 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.07843138 0.75294125 0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.9960785  0.5294118  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.01568628 0.2509804  0.6313726  0.6313726  0.6313726\n",
      " 0.8980393  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.75294125 0.8000001  0.9960785  0.9960785  0.9960785  0.7843138\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.1764706  0.2392157\n",
      " 0.79215693 0.8470589  0.8470589  0.30980393 0.07843138 0.10980393\n",
      " 0.8862746  0.9960785  0.9960785  0.3254902  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.85098046 0.9960785\n",
      " 0.9960785  0.18039216 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.43529415 0.98823535 0.9960785  0.9960785  0.18039216\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.45882356\n",
      " 0.9960785  0.9960785  0.6862745  0.0509804  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.45882356 0.9960785  1.\n",
      " 0.79215693 0.09411766 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.45882356 0.9960785  0.9960785  0.86666673 0.1254902\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.45882356\n",
      " 0.9960785  0.9960785  0.57254905 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.45882356 0.9960785  0.9960785\n",
      " 0.57254905 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.45882356 0.9960785  0.9960785  0.57254905 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.04313726 0.8196079\n",
      " 0.9960785  0.97647065 0.19215688 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.0627451  0.9960785  0.9960785  0.6901961\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.5803922  0.9960785  0.9960785  0.36078432 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.01176471 0.6784314  0.9960785\n",
      " 0.9960785  0.36078432 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.27450982 0.9960785  0.9960785  0.9960785  0.36078432\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.27450982\n",
      " 0.9960785  0.9960785  0.9450981  0.28627452 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.10980393 0.7960785  1.\n",
      " 0.38431376 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(input_batch[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We cannot predict the digit from the above\n",
    "### we need to plot a image from the data above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SY--gEoPGVQa",
    "outputId": "0e0af936-fb49-4e87-b1c2-e6c87fd65ab9"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch_vis, target_batch_vis = mnist.train.next_batch(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 504x504 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "gimg=input_batch_vis[1].reshape(28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(gimg,cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### so we can conclude that above number is same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_batch_vis[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So we can verify position at target_batch is same as the digit (as target batch is one hot encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now lets check if our model predict the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_batch_vis[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-25-8499056eb141>:1: Print (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2018-08-20.\n",
      "Instructions for updating:\n",
      "Use tf.print instead of tf.Print. Note that tf.print returns a no-output operator that directly prints the output. Outside of defuns or eager mode, this operator will not be executed unless it is directly specified in session.run or used as a control dependency for other operators. This is only a concern in graph mode. Below is an example of how to ensure tf.print executes in graph mode:\n",
      "```python\n",
      "    sess = tf.Session()\n",
      "    with sess.as_default():\n",
      "        tensor = tf.range(10)\n",
      "        print_op = tf.print(tensor)\n",
      "        with tf.control_dependencies([print_op]):\n",
      "          out = tf.add(tensor, tensor)\n",
      "        sess.run(out)\n",
      "    ```\n",
      "Additionally, to use tf.print in python 2.7, users must make sure to import\n",
      "the following:\n",
      "\n",
      "  `from __future__ import print_function`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prints=tf.Print(target_batch_vis[1],[target_batch_vis[1]],\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run([prints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = sess.run([prints], \n",
    "                          feed_dict={inputs: input_batch_vis, targets: target_batch_vis}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## so the output is same is the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Untitled0.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
